{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95e3ae5",
   "metadata": {},
   "source": [
    "# Assignment 5: Visual Servoing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0fff4",
   "metadata": {},
   "source": [
    "#### for every question this code need a restart to reset the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ec1fb",
   "metadata": {},
   "source": [
    "The following has ALL the code to run the simulation and all the problems. You do not need to understand exactly how it works. You only need to work on the sections starting at \"COMPLETE THIS\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaab71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "from scipy.spatial.transform import Rotation as Rot            #can use this to apply angular rotations to coordinate frames\n",
    "\n",
    "#camera (don't change these settings)\n",
    "camera_width = 512                                             #image width\n",
    "camera_height = 512                                            #image height\n",
    "camera_fov = 120                                                #field of view of camera\n",
    "camera_focal_depth = 0.5*camera_height/np.tan(0.5*np.pi/180*camera_fov) \n",
    "                                                               #focal depth in pixel space\n",
    "camera_aspect = camera_width/camera_height                     #aspect ratio\n",
    "camera_near = 0.02                                             #near clipping plane in meters, do not set non-zero\n",
    "camera_far = 100                                               #far clipping plane in meters\n",
    "\n",
    "\n",
    "#control objectives (if you wish, you can play with these values for fun)\n",
    "object_location_desired = np.array([camera_width/2,camera_height/2])\n",
    "\n",
    "                                                               #center the object to middle of image \n",
    "K_p_x = 0.1                                                   #Proportional control gain for translation\n",
    "K_p_Omega = 0.02                                              #Proportional control gain for rotation       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c20c1b",
   "metadata": {},
   "source": [
    "### Create the Robot Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39edfda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robot with Camera Class\n",
    "class eye_in_hand_robot:\n",
    "    def get_ee_position(self):\n",
    "        '''\n",
    "        Function to return the end-effector of the link. This is the very tip of the robot at the end of the jaws.\n",
    "        '''\n",
    "        endEffectorIndex = self.numActiveJoints\n",
    "        endEffectorState = p.getLinkState(self.robot_id, endEffectorIndex)\n",
    "        endEffectorPos = np.array(endEffectorState[0])\n",
    "        endEffectorOrn = np.array(p.getMatrixFromQuaternion(endEffectorState[1])).reshape(3,3)\n",
    "        \n",
    "        #add an offset to get past the forceps\n",
    "        endEffectorPos += self.camera_offset*endEffectorOrn[:,2]\n",
    "        return endEffectorPos, endEffectorOrn\n",
    "\n",
    "    def get_current_joint_angles(self):\n",
    "        # Get the current joint angles\n",
    "        joint_angles = np.zeros(self.numActiveJoints)\n",
    "        for i in range(self.numActiveJoints):\n",
    "            joint_state = p.getJointState(self.robot_id, self._active_joint_indices[i])\n",
    "            joint_angles[i] = joint_state[0]\n",
    "        return joint_angles\n",
    "    \n",
    "    def get_jacobian_at_current_position(self):\n",
    "        #Returns the Robot Jacobian of the last active link\n",
    "        mpos, mvel, mtorq = self.get_active_joint_states()   \n",
    "        zero_vec = [0.0]*len(mpos)\n",
    "        linearJacobian, angularJacobian = p.calculateJacobian(self.robot_id, \n",
    "                                                              self.numActiveJoints,\n",
    "                                                              [0,0,self.camera_offset],\n",
    "                                                              mpos, \n",
    "                                                              zero_vec,\n",
    "                                                              zero_vec)\n",
    "        #only return the active joint's jacobians\n",
    "        Jacobian = np.vstack((linearJacobian,angularJacobian))\n",
    "        return Jacobian[:,:self.numActiveJoints]\n",
    "    \n",
    "    def set_joint_position(self, desireJointPositions, kp=1.0, kv=0.3):\n",
    "        '''Set  the joint angle positions of the robot'''\n",
    "        zero_vec = [0.0] * self._numLinkJoints\n",
    "        allJointPositionObjectives = [0.0]*self._numLinkJoints\n",
    "        for i in range(desireJointPositions.shape[0]):\n",
    "            idx = self._active_joint_indices[i]\n",
    "            allJointPositionObjectives[idx] = desireJointPositions[i]\n",
    "\n",
    "        p.setJointMotorControlArray(self.robot_id,\n",
    "                                    range(self._numLinkJoints),\n",
    "                                    p.POSITION_CONTROL,\n",
    "                                    targetPositions=allJointPositionObjectives,\n",
    "                                    targetVelocities=zero_vec,\n",
    "                                    positionGains=[kp] * self._numLinkJoints,\n",
    "                                    velocityGains=[kv] * self._numLinkJoints)\n",
    "\n",
    "    def get_active_joint_states(self):\n",
    "        '''Get the states (position, velocity, and torques) of the active joints of the robot\n",
    "        '''\n",
    "        joint_states = p.getJointStates(self.robot_id, range(self._numLinkJoints))\n",
    "        joint_infos = [p.getJointInfo(self.robot_id, i) for i in range(self._numLinkJoints)]\n",
    "        joint_states = [j for j, i in zip(joint_states, joint_infos) if i[3] > -1]\n",
    "        joint_positions = [state[0] for state in joint_states]\n",
    "        joint_velocities = [state[1] for state in joint_states]\n",
    "        joint_torques = [state[3] for state in joint_states]\n",
    "        return joint_positions, joint_velocities, joint_torques\n",
    "\n",
    "\n",
    "         \n",
    "    def __init__(self, robot_id, initialJointPos):\n",
    "        self.robot_id = robot_id\n",
    "        self.eeFrameId = []\n",
    "        self.camera_offset = 0.1 #offset camera in z direction to avoid grippers\n",
    "        # Get the joint info\n",
    "        self._numLinkJoints = p.getNumJoints(self.robot_id) #includes passive joint\n",
    "        jointInfo = [p.getJointInfo(self.robot_id, i) for i in range(self._numLinkJoints)]\n",
    "        \n",
    "        # Get joint locations (some joints are passive)\n",
    "        self._active_joint_indices = []\n",
    "        for i in range(self._numLinkJoints):\n",
    "            if jointInfo[i][2]==p.JOINT_REVOLUTE:\n",
    "                self._active_joint_indices.append(jointInfo[i][0])\n",
    "        self.numActiveJoints = len(self._active_joint_indices) #exact number of active joints\n",
    "\n",
    "        #reset joints\n",
    "        for i in range(self._numLinkJoints):\n",
    "            p.resetJointState(self.robot_id,i,initialJointPos[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99913072",
   "metadata": {},
   "source": [
    "### Ancilliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_coordinate_frame(position, orientation, length, frameId = []):\n",
    "    '''\n",
    "    Draws a coordinate frame x,y,z with scaled lengths on the axes \n",
    "    in a position and orientation relative to the world coordinate frame\n",
    "    pos: 3-element numpy array\n",
    "    orientation: 3x3 numpy matrix\n",
    "    length: length of the plotted x,y,z axes\n",
    "    frameId: a unique ID for the frame. If this supplied, then it will erase the previous location of the frame\n",
    "    \n",
    "    returns the frameId\n",
    "    '''\n",
    "    if len(frameId)!=0:\n",
    "        p.removeUserDebugItem(frameId[0])\n",
    "        p.removeUserDebugItem(frameId[1])\n",
    "        p.removeUserDebugItem(frameId[2])\n",
    "    \n",
    "    lineIdx=p.addUserDebugLine(position, position + np.dot(orientation, [length, 0, 0]), [1, 0, 0])  # x-axis in red\n",
    "    lineIdy=p.addUserDebugLine(position, position + np.dot(orientation, [0, length, 0]), [0, 1, 0])  # y-axis in green\n",
    "    lineIdz=p.addUserDebugLine(position, position + np.dot(orientation, [0, 0, length]), [0, 0, 1])  # z-axis in blue\n",
    "\n",
    "    return lineIdx,lineIdy,lineIdz\n",
    "\n",
    "def opengl_plot_world_to_pixelspace(pt_in_3D_to_project, viewMat, projMat, imgWidth, imgHeight):\n",
    "    ''' Plots a x,y,z location in the world in an openCV image\n",
    "    This is used for debugging, e.g. given a known location in the world, verify it appears in the camera\n",
    "    when using p.getCameraImage(...). The output [u,v], when plot with opencv, should line up with object \n",
    "    in the image from p.getCameraImage(...)\n",
    "    '''\n",
    "    pt_in_3D_to_project = np.append(pt_in_3D_to_project,1)\n",
    "    #print('Point in 3D to project:', pt_in_3D_to_project)\n",
    "\n",
    "    pt_in_3D_in_camera_frame = viewMat @ pt_in_3D_to_project\n",
    "    #print('Point in camera space: ', pt_in_3D_in_camera_frame)\n",
    "\n",
    "    # Convert coordinates to get normalized device coordinates (before rescale)\n",
    "    uvzw = projMat @ pt_in_3D_in_camera_frame\n",
    "    #print('after projection: ', uvzw)\n",
    "\n",
    "    # scale to get the normalized device coordinates\n",
    "    uvzw_NDC = uvzw/uvzw[3]\n",
    "    #print('after normalization: ', uvzw_NDC)\n",
    "\n",
    "    #x,y specifies lower left corner of viewport rectangle, in pixels. initial value is (0,)\n",
    "    u = ((uvzw_NDC[0] + 1) / 2.0) * imgWidth\n",
    "    v = ((1-uvzw_NDC[1]) / 2.0) * imgHeight\n",
    "\n",
    "    return [int(u),int(v)]\n",
    "\n",
    "    \n",
    "def get_camera_view_and_projection_opencv(cameraPos, camereaOrn):\n",
    "    '''Gets the view and projection matrix for a camera at position (3) and orientation (3x3)'''\n",
    "    __camera_view_matrix_opengl = p.computeViewMatrix(cameraEyePosition=cameraPos,\n",
    "                                                   cameraTargetPosition=cameraPos+camereaOrn[:,2],\n",
    "                                                   cameraUpVector=-camereaOrn[:,1])\n",
    "\n",
    "    __camera_projection_matrix_opengl = p.computeProjectionMatrixFOV(camera_fov, camera_aspect, camera_near, camera_far)        \n",
    "    _, _, rgbImg, depthImg, _ = p.getCameraImage(camera_width, \n",
    "                                                 camera_height, \n",
    "                                                 __camera_view_matrix_opengl,\n",
    "                                                 __camera_projection_matrix_opengl, \n",
    "                                                 renderer=p.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "    #returns camera view and projection matrices in a form that fits openCV\n",
    "    viewMat = np.array(__camera_view_matrix_opengl).reshape(4,4).T\n",
    "    projMat = np.array(__camera_projection_matrix_opengl).reshape(4,4).T\n",
    "    return viewMat, projMat\n",
    "\n",
    "def get_camera_img_float(cameraPos, camereaOrn):\n",
    "    ''' Gets the image and depth map from a camera at a position cameraPos (3) and cameraOrn (3x3) in space. '''\n",
    "    __camera_view_matrix_opengl = p.computeViewMatrix(cameraEyePosition=cameraPos,\n",
    "                                                   cameraTargetPosition=cameraPos+camereaOrn[:,2],\n",
    "                                                   cameraUpVector=-camereaOrn[:,1])\n",
    "\n",
    "    __camera_projection_matrix_opengl = p.computeProjectionMatrixFOV(camera_fov, camera_aspect, camera_near, camera_far)        \n",
    "    width, height, rgbImg, nonlinDepthImg, _ = p.getCameraImage(camera_width, \n",
    "                                                 camera_height, \n",
    "                                                 __camera_view_matrix_opengl,\n",
    "                                                 __camera_projection_matrix_opengl, \n",
    "                                                 renderer=p.ER_BULLET_HARDWARE_OPENGL)\n",
    "\n",
    "    #adjust for clipping and nonlinear distance i.e., 1/d (0 is closest, i.e., near, 1 is furthest away, i.e., far\n",
    "    depthImgLinearized =camera_far*camera_near/(camera_far+camera_near-(camera_far-camera_near)*nonlinDepthImg)\n",
    "\n",
    "    #convert to numpy and a rgb-d image\n",
    "    rgb_image = np.array(rgbImg[:,:,:3], dtype=np.uint8)\n",
    "    depth_image = np.array(depthImgLinearized, dtype=np.float32)\n",
    "    return rgb_image, depth_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd326314",
   "metadata": {},
   "source": [
    "### Create Physics Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the connection to the physics server\n",
    "physicsClient = p.connect(p.GUI)#(p.DIRECT)\n",
    "time_step = 0.001\n",
    "p.resetSimulation()\n",
    "p.setTimeStep(time_step)\n",
    "p.setGravity(0, 0, -9.8)\n",
    "\n",
    "# Set the path to the URDF files included with PyBullet\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "# load a plane URDF\n",
    "p.loadURDF('plane.urdf')\n",
    "\n",
    "# Place obstacles\n",
    "box_length = box_width = 0.4\n",
    "box_depth = 0.02\n",
    "object_center = [0.3, 1, 0.01]\n",
    "object_orientation = [0, 0, 0]\n",
    "object_color = [0.8, 0.0, 0.0, 1]\n",
    "geomBox = p.createCollisionShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2])\n",
    "visualBox = p.createVisualShape(p.GEOM_BOX, halfExtents=[box_length/2, box_width/2, box_depth/2], rgbaColor=object_color)\n",
    "boxId = p.createMultiBody(\n",
    "    baseMass=0,\n",
    "    baseCollisionShapeIndex=geomBox,\n",
    "    baseVisualShapeIndex=visualBox,\n",
    "    basePosition=np.array(object_center),\n",
    "    baseOrientation=p.getQuaternionFromEuler(object_orientation)\n",
    ")\n",
    "ObjectModelPos, modelOrn  = p.getBasePositionAndOrientation(boxId)\n",
    "\n",
    "#reset debug gui camera position so we can see the robot up close\n",
    "p.resetDebugVisualizerCamera( cameraDistance=1, cameraYaw=30, cameraPitch=-52, cameraTargetPosition=[0,0,.5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5bb297",
   "metadata": {},
   "source": [
    "### COMPLETE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageJacobian(u_px, v_px, depthImg, f, imgWidth, imgHeight, cameraOrientation):\n",
    "    u_idx = int(np.clip(round(u_px), 0, depthImg.shape[1] - 1))\n",
    "    v_idx = int(np.clip(round(v_px), 0, depthImg.shape[0] - 1))\n",
    "    Z = float(depthImg[v_idx, u_idx])\n",
    "    if Z < 0.01:\n",
    "        Z = 0.01\n",
    "\n",
    "    print(\"u_idx, v_idx, Z\", u_idx, v_idx, Z)\n",
    "    x = (u_px - imgWidth/2.0) / f\n",
    "    y = (v_px - imgHeight/2.0) / f\n",
    "\n",
    "    L_norm = np.array([\n",
    "        [-1.0/Z,  0.0,    x/Z,          x*y,          -(1.0 + x*x),   y],\n",
    "        [ 0.0,   -1.0/Z,  y/Z,          1.0 + y*y,    -x*y,          -x]\n",
    "    ], dtype=float)\n",
    "\n",
    "    L_cam = np.empty_like(L_norm)\n",
    "    L_cam[0,:] =  f * L_norm[0,:]\n",
    "    L_cam[1,:] =  f * L_norm[1,:]\n",
    "\n",
    "    # camera to world   \n",
    "    R_wc = np.asarray(cameraOrientation, dtype=float)\n",
    "    R_cw = R_wc.T\n",
    "    T = np.block([\n",
    "        [R_cw,            np.zeros((3,3))],\n",
    "        [np.zeros((3,3)), R_cw]\n",
    "    ])\n",
    "\n",
    "    L_world = L_cam @ T\n",
    "\n",
    "    return L_world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9940a0b0",
   "metadata": {},
   "source": [
    "#### comment there to solve only translation or rotation in Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f59d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCameraControl(object_loc_des, object_loc, image_jacobian):    \n",
    "    error = np.array(object_loc_des) - np.array(object_loc)\n",
    "    \n",
    "    # Use only translation\n",
    "    # image_jacobian[:, 3:6] = 0\n",
    "    \n",
    "    # Use only rotation\n",
    "    # image_jacobian[:, 0:3] = 0\n",
    "        \n",
    "    J_pseudo_inv = np.linalg.pinv(image_jacobian) \n",
    "    \n",
    "    camera_velocity = J_pseudo_inv @ error\n",
    "    \n",
    "    velocity_trans = camera_velocity[0:3]  \n",
    "    velocity_rot = camera_velocity[3:6]   \n",
    "    \n",
    "    delta_X = K_p_x * velocity_trans          \n",
    "    delta_Omega = K_p_Omega * velocity_rot    \n",
    "    return delta_X, delta_Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findJointControl(robot, delta_X, delta_Omega):\n",
    "    current_joint_angles = robot.get_current_joint_angles()\n",
    "    \n",
    "    J_robot = robot.get_jacobian_at_current_position()\n",
    "    \n",
    "    camera_velocity = np.hstack((delta_X, delta_Omega))\n",
    "    \n",
    "    joint_velocity = np.linalg.pinv(J_robot) @ camera_velocity\n",
    "    \n",
    "    new_jointPositions = current_joint_angles + joint_velocity\n",
    "    \n",
    "    return new_jointPositions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41de24",
   "metadata": {},
   "source": [
    "## SIMULATE ANSWERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fac3dc4",
   "metadata": {},
   "source": [
    "### Q1 and Q2 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_pos_init = np.array([0.2, 0.0, 1.0])\n",
    "cam_rot_init = np.array([[1,0,0],[0,-1,0],[0,0,-1]])\n",
    "axes = [\n",
    "    (\"+vx\", 0),\n",
    "    (\"+vy\", 1),\n",
    "    (\"+vz\", 2),\n",
    "    (\"+wx\", 3),\n",
    "    (\"+wy\", 4),\n",
    "    (\"+wz\", 5),\n",
    "]\n",
    "\n",
    "camera_frameId = []\n",
    "\n",
    "for name, id in axes:\n",
    "    print(f\"\\n=== Axis: {name} ===\")\n",
    "    log_id = p.startStateLogging(p.STATE_LOGGING_VIDEO_MP4, f\"Q1_{name}.mp4\")\n",
    "    cameraPosition    = cam_pos_init.copy()\n",
    "    cameraOrientation = cam_rot_init.copy()\n",
    "\n",
    "    for t in range(20):\n",
    "        p.stepSimulation()\n",
    "\n",
    "        rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "        viewMat, projMat = get_camera_view_and_projection_opencv(cameraPosition, cameraOrientation)\n",
    "        object_loc = opengl_plot_world_to_pixelspace(object_center, viewMat, projMat, camera_width, camera_height)\n",
    "\n",
    "        L = getImageJacobian(object_loc[0], object_loc[1], depth, camera_focal_depth,\n",
    "                             camera_width, camera_height, cameraOrientation)\n",
    "        delta_X, delta_Omega = findCameraControl(object_location_desired, object_loc, L)\n",
    "        control = np.concatenate((delta_X, delta_Omega), axis=0)\n",
    "        L_inv = np.zeros((6,))\n",
    "\n",
    "        # enlarger the control\n",
    "        L_inv[id] = control[id] * 10\n",
    "        delta_X = L_inv[0:3]\n",
    "        delta_Omega = L_inv[3:6]\n",
    "\n",
    "        cameraPosition = cameraPosition + delta_X\n",
    "\n",
    "        omega_mag = np.linalg.norm(delta_Omega)\n",
    "        if omega_mag > 1e-9:\n",
    "            rotation_update   = Rot.from_rotvec(delta_Omega)\n",
    "            current_rotation  = Rot.from_matrix(cameraOrientation)\n",
    "            cameraOrientation = (rotation_update * current_rotation).as_matrix()\n",
    "\n",
    "        if t % 1 == 0:\n",
    "            camera_frameId = draw_coordinate_frame(cameraPosition, cameraOrientation, \n",
    "                                                length=0.15, frameId=camera_frameId)\n",
    "        \n",
    "        if t % 1 == 0:\n",
    "            error = np.array(object_location_desired) - np.array(object_loc)\n",
    "            print(f\"Iter {t}: Error={np.linalg.norm(error):.2f}px, Obj@{object_loc}, Des@{object_location_desired}\")\n",
    "\n",
    "        # cv2.imshow(\"depth\", depth)\n",
    "        # cv2.imshow(\"rgb\", rgb)\n",
    "        # cv2.waitKey(1)\n",
    "        \n",
    "    p.stopStateLogging(log_id)\n",
    "cv2.destroyAllWindows()\n",
    "p.disconnect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b55393",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameraPosition = np.array([0.2,0,1])\n",
    "cameraOrientation = np.array([[1,0,0],[0,-1,0],[0,0,-1]])\n",
    "\n",
    "camera_frameId = []\n",
    "log_id = p.startStateLogging(p.STATE_LOGGING_VIDEO_MP4, \"test.mp4\")\n",
    "\n",
    "for ITER in range(100):\n",
    "    p.stepSimulation()\n",
    " \n",
    "    ''' Get Image'''\n",
    "    rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "    \n",
    "    ''' Magical Computer Vision Algorithm that gets locations of objects in the image, as object_loc (Do Not Remove)'''\n",
    "    viewMat, projMat = get_camera_view_and_projection_opencv(cameraPosition, cameraOrientation)\n",
    "    object_loc = opengl_plot_world_to_pixelspace(object_center, viewMat, projMat,camera_width, camera_height)\n",
    "    \n",
    "    ''' Get Image Jacobian '''\n",
    "    imageJacobian = getImageJacobian(object_loc[0], object_loc[1], depth, camera_focal_depth, camera_width, camera_height, cameraOrientation)\n",
    "\n",
    "    ''' Get Camera Control '''\n",
    "    delta_X, delta_Omega = findCameraControl(object_location_desired, object_loc, imageJacobian)\n",
    "\n",
    "    cameraPosition = cameraPosition + delta_X\n",
    "\n",
    "    omega_mag = np.linalg.norm(delta_Omega)\n",
    "    if omega_mag > 1e-6:\n",
    "        rotation_update   = Rot.from_rotvec(delta_Omega)  # 世界系旋量\n",
    "        current_rotation  = Rot.from_matrix(cameraOrientation)\n",
    "        cameraOrientation = (rotation_update * current_rotation).as_matrix()\n",
    "    \n",
    "    if ITER % 1 == 0:\n",
    "        camera_frameId = draw_coordinate_frame(cameraPosition, cameraOrientation, \n",
    "                                               length=0.15, frameId=camera_frameId)\n",
    "    \n",
    "    if ITER % 1 == 0:\n",
    "        error = np.array(object_location_desired) - np.array(object_loc)\n",
    "        print(f\"Iter {ITER}: Error={np.linalg.norm(error):.2f}px, Obj@{object_loc}, Des@{object_location_desired}\")\n",
    "\n",
    "    # # show image\n",
    "    # cv2.imshow(\"depth\", depth)\n",
    "    # cv2.imshow(\"rgb\", rgb)\n",
    "    # cv2.waitKey(1)\n",
    "    \n",
    "p.stopStateLogging(log_id)\n",
    "\n",
    "#close the physics server\n",
    "cv2.destroyAllWindows()    \n",
    "p.disconnect() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cb60e",
   "metadata": {},
   "source": [
    "### Q3_a(object not moving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f509e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create Robot Instance'''\n",
    "pandaUid = p.loadURDF(os.path.join(pybullet_data.getDataPath(), \"franka_panda/panda.urdf\"),useFixedBase=True)\n",
    "p.resetBasePositionAndOrientation(pandaUid, [0, 0, 0], [0, 0, 0, 1])\n",
    "initialJointPosition = [0,-np.pi/4,np.pi/4,-np.pi/4,np.pi/4,np.pi/4,np.pi/4,0,0,0,0,0]\n",
    "robot = eye_in_hand_robot(pandaUid,initialJointPosition)\n",
    "p.stepSimulation() # need to do this to initialize robot\n",
    "\n",
    "log_id = p.startStateLogging(p.STATE_LOGGING_VIDEO_MP4, \"Q3_stationary.mp4\")\n",
    "camera_frameId = []\n",
    "\n",
    "for ITER in range(200):\n",
    "    p.stepSimulation()\n",
    " \n",
    "    ''' Match Camera Pose to Robot End-Effector and Get Image'''\n",
    "    cameraPosition, cameraOrientation = robot.get_ee_position()\n",
    "\n",
    "    camera_frameId = draw_coordinate_frame(cameraPosition, cameraOrientation, \n",
    "                                        length=0.15, frameId=camera_frameId)\n",
    "    \n",
    "    rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "    \n",
    "    ''' Magical Computer Vision Algorithm that gets locations of objects in the image, as object_loc (Do Not Remove)'''\n",
    "    viewMat, projMat = get_camera_view_and_projection_opencv(cameraPosition, cameraOrientation)\n",
    "    object_loc = opengl_plot_world_to_pixelspace(object_center, viewMat, projMat,camera_width, camera_height)\n",
    "\n",
    "    imageJacobian = getImageJacobian(object_loc[0], object_loc[1], depth, \n",
    "                                      camera_focal_depth, camera_width, camera_height, cameraOrientation)\n",
    "    \n",
    "    delta_X, delta_Omega = findCameraControl(object_location_desired, object_loc, imageJacobian)\n",
    "    delta_X = delta_X * 0.1\n",
    "    delta_Omega = delta_Omega * 0.1\n",
    "    \n",
    "    new_jointPositions = findJointControl(robot, delta_X, delta_Omega)\n",
    "    \n",
    "    robot.set_joint_position(new_jointPositions)\n",
    "    \n",
    "    if ITER % 10 == 0:\n",
    "        error = np.linalg.norm(np.array(object_location_desired) - np.array(object_loc))\n",
    "        print(f\"Iter {ITER}: Error={error:.2f}px, Object位置={object_loc}, 目标位置={object_location_desired.tolist()}\")\n",
    "    \n",
    "    cv2.imshow(\"depth\", depth)\n",
    "    cv2.imshow(\"rgb\", rgb)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "p.stopStateLogging(log_id)\n",
    "\n",
    "#close the physics server\n",
    "cv2.destroyAllWindows()    \n",
    "p.disconnect() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5bd7cd",
   "metadata": {},
   "source": [
    "### Q3_b(Track Moving Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c7aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create Robot Instance'''\n",
    "pandaUid = p.loadURDF(os.path.join(pybullet_data.getDataPath(), \"franka_panda/panda.urdf\"),useFixedBase=True)\n",
    "p.resetBasePositionAndOrientation(pandaUid, [0, 0, 0], [0, 0, 0, 1])\n",
    "initialJointPosition = [0,-np.pi/4,np.pi/4,-np.pi/4,np.pi/4,np.pi/4,np.pi/4,0,0,0,0,0]\n",
    "robot = eye_in_hand_robot(pandaUid,initialJointPosition)\n",
    "p.stepSimulation() # need to do this to initialize robot\n",
    "\n",
    "log_id = p.startStateLogging(p.STATE_LOGGING_VIDEO_MP4, \"Q3_moving.mp4\")\n",
    "cameraframeId = []\n",
    "\n",
    "for ITER in range(300):\n",
    "    p.stepSimulation()\n",
    "    \n",
    "    object_center=[0.5+0.2*np.sin(np.pi/2+ITER/10), 0.5+0.2*np.sin(ITER/20), 0.01]\n",
    "    p.resetBasePositionAndOrientation(boxId,object_center,p.getQuaternionFromEuler(object_orientation))\n",
    " \n",
    "    ''' Match Camera Pose to Robot End-Effector and Get Image'''\n",
    "    cameraPosition, cameraOrientation = robot.get_ee_position()\n",
    "    \n",
    "    rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "    \n",
    "    ''' Magical Computer Vision Algorithm that gets locations of objects in the image, as object_loc (Do Not Remove)'''\n",
    "    viewMat, projMat = get_camera_view_and_projection_opencv(cameraPosition, cameraOrientation)\n",
    "    object_loc = opengl_plot_world_to_pixelspace(object_center, viewMat, projMat,camera_width, camera_height)\n",
    "\n",
    "    ''' Do some things here to get robot control'''\n",
    "    imageJacobian = getImageJacobian(object_loc[0], object_loc[1], depth, \n",
    "                                      camera_focal_depth, camera_width, camera_height, cameraOrientation)\n",
    "    \n",
    "    delta_X, delta_Omega = findCameraControl(object_location_desired, object_loc, imageJacobian)\n",
    "    delta_X = delta_X * 0.3\n",
    "    delta_Omega = delta_Omega * 0.3\n",
    "    \n",
    "    new_jointPositions = findJointControl(robot, delta_X, delta_Omega)\n",
    "    \n",
    "    robot.set_joint_position(new_jointPositions)\n",
    "    \n",
    "    if ITER % 10 == 0:\n",
    "        error = np.linalg.norm(np.array(object_location_desired) - np.array(object_loc))\n",
    "        print(f\"Iter {ITER}: Error={error:.2f}px, Object location={object_loc}, Target location={object_location_desired.tolist()}\")\n",
    "    \n",
    "    cv2.imshow(\"depth\", depth)\n",
    "    cv2.imshow(\"rgb\", rgb)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# 停止录制\n",
    "p.stopStateLogging(log_id)\n",
    "\n",
    "#close the physics server\n",
    "cv2.destroyAllWindows()    \n",
    "p.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a4128",
   "metadata": {},
   "source": [
    "### Q3_c(Track Moving Target with no roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roll_twist_sec(cameraOrientation, k_roll=1.0):\n",
    "    \"\"\"\n",
    "    Compute the secondary roll twist for the camera to maintain zero roll \n",
    "    with respect to the world z-axis.\n",
    "\n",
    "    Args:\n",
    "        cameraOrientation (np.ndarray): The 3x3 rotation matrix representing the camera orientation in world coordinates.\n",
    "        k_roll (float, optional): Gain parameter for roll correction. Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "        psi (float): Roll angle (in radians) of the camera around the viewing axis (z) with respect to the world z-axis.\n",
    "        v_sec_world (np.ndarray): 6D twist vector in the world frame, representing the desired secondary control to minimize roll.\n",
    "                                 The first 3 elements are zeros (no translation), and the last 3 are the angular velocity.\n",
    "    \"\"\"\n",
    "    R_wc = np.asarray(cameraOrientation, dtype=float)   \n",
    "    xw, yw, zw = R_wc[:,0], R_wc[:,1], R_wc[:,2]      \n",
    "    g  = np.array([0.0, 0.0, 1.0])                    \n",
    "    g_perp = g - np.dot(g, zw) * zw                \n",
    "    nrm = np.linalg.norm(g_perp)\n",
    "    if nrm < 1e-9:\n",
    "        return 0.0, np.zeros(6)                       \n",
    "    g_perp /= nrm\n",
    "\n",
    "    psi = np.arctan2(np.dot(xw, g_perp), np.dot(yw, g_perp))\n",
    "    omega_sec = -k_roll * psi * zw                    \n",
    "    v_sec_world = np.zeros(6)\n",
    "    v_sec_world[3:] = omega_sec\n",
    "    return psi, v_sec_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b7de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create Robot Instance'''\n",
    "pandaUid = p.loadURDF(os.path.join(pybullet_data.getDataPath(), \"franka_panda/panda.urdf\"),useFixedBase=True)\n",
    "p.resetBasePositionAndOrientation(pandaUid, [0, 0, 0], [0, 0, 0, 1])\n",
    "initialJointPosition = [0,-np.pi/4,np.pi/4,-np.pi/4,np.pi/4,np.pi/4,np.pi/4,0,0,0,0,0]\n",
    "robot = eye_in_hand_robot(pandaUid,initialJointPosition)\n",
    "p.stepSimulation() # need to do this to initialize robot\n",
    "\n",
    "log_id = p.startStateLogging(p.STATE_LOGGING_VIDEO_MP4, \"Q3_no_roll.mp4\")\n",
    "cameraframeId = []\n",
    "\n",
    "for ITER in range(300):\n",
    "    p.stepSimulation()\n",
    "    \n",
    "    object_center=[0.5+0.2*np.sin(np.pi/2+ITER/10), 0.5+0.2*np.sin(ITER/20), 0.01]\n",
    "    p.resetBasePositionAndOrientation(boxId,object_center,p.getQuaternionFromEuler(object_orientation))\n",
    " \n",
    "    ''' Match Camera Pose to Robot End-Effector and Get Image'''\n",
    "    cameraPosition, cameraOrientation = robot.get_ee_position()\n",
    "    \n",
    "    rgb, depth = get_camera_img_float(cameraPosition, cameraOrientation)\n",
    "    \n",
    "    ''' Magical Computer Vision Algorithm that gets locations of objects in the image, as object_loc (Do Not Remove)'''\n",
    "    viewMat, projMat = get_camera_view_and_projection_opencv(cameraPosition, cameraOrientation)\n",
    "    object_loc = opengl_plot_world_to_pixelspace(object_center, viewMat, projMat,camera_width, camera_height)\n",
    "\n",
    "    ''' Do some things here to get robot control'''\n",
    "    L_world = getImageJacobian(object_loc[0], object_loc[1], depth,\n",
    "                                    camera_focal_depth, camera_width, camera_height,\n",
    "                                    cameraOrientation)              # 2x6\n",
    "\n",
    "    delta_X, delta_Omega = findCameraControl(object_location_desired, object_loc, L_world)\n",
    "    delta = np.concatenate((delta_X, delta_Omega))\n",
    "    kr = 1.0\n",
    "    psi, v_sec_raw_world = compute_roll_twist_sec(cameraOrientation, k_roll=kr)\n",
    "\n",
    "    Lplus = np.linalg.pinv(L_world)\n",
    "    N = np.eye(6) - Lplus @ L_world\n",
    "    v_cmd_world = delta + N @ v_sec_raw_world                 # 6,\n",
    "\n",
    "    v_cmd_world[:3]  *= 0.1\n",
    "    v_cmd_world[3:]  *= 0.1  \n",
    "\n",
    "    delta_X_world     = v_cmd_world[:3]\n",
    "    delta_Omega_world = v_cmd_world[3:]\n",
    "    new_jointPositions = findJointControl(robot, delta_X_world, delta_Omega_world)\n",
    "\n",
    "    robot.set_joint_position(new_jointPositions)\n",
    "\n",
    "    if ITER % 10 == 0:\n",
    "        error = np.linalg.norm(np.array(object_location_desired) - np.array(object_loc))\n",
    "        print(f\"Iter {ITER}: Error={error:.2f}px, Object位置={object_loc}, 目标位置={object_location_desired.tolist()}\")\n",
    "    \n",
    "    cv2.imshow(\"depth\", depth)\n",
    "    cv2.imshow(\"rgb\", rgb)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# 停止录制\n",
    "p.stopStateLogging(log_id)\n",
    "\n",
    "#close the physics server\n",
    "cv2.destroyAllWindows()    \n",
    "p.disconnect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECE276C",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
